import numpy as np
import math

class MLP_Complete:

    """
    Constructor: Computes MLP_Complete.
    Args:
        inputLayer (int): size of input
        hiddenLayers (array-like): number
        of layers and size of each layers.
        outputLayer (int): size of output layer
        seed (scalar): seed of the random numeric.
        epislom (scalar) : random initialization range.
        e.j: 1 = [-1..1], 2 = [-2,2]...
    """
    def __init__(self,inputLayer, hiddenLayers, outputLayer,
    seed=0, epislom = 0.12):
        ## TO-DO

        self.inputLayer = inputLayer
        self.hiddenLayer = hiddenLayers
        self.outputLayer = outputLayer
        self.epsilom = epislom
        self.seed = seed

        # 
        # sizes de las capas en una lista con todooos los sizes
        self.layer_size = [inputLayer]
        for i in hiddenLayers:             
            self.layer_size += [i]
        self.layer_size += [outputLayer]

        # preapara las thetas (lol)
        self.thetas = []
        self.new_trained(self.thetas, self.epsilom)

        """
    Reset the theta matrix created in the constructor by both theta matrix manualy loaded.

    Args:
        theta1 (array_like): Weights for the first layer in the neural network.
        theta2 (array_like): Weights for the second layer in the neural network.
    """
    def new_trained(self,thetas,epsilom):
        for i in range(len(self.layer_size) - 1):
            # coge las capas de entrada y salida
            in_layer = self.layer_size[i]
            out_layer = self.layer_size[i + 1]

            theta = np.random.uniform(-epsilom, epsilom, (out_layer, in_layer + 1))
            thetas.append(theta)
        
    """
    Num elements in the training data. (private)

    Args:
        x (array_like): input data. 
    """
    def _size(self,x):
        return x.shape[0]
    
    """
    Computes de sigmoid function of z (private)

    Args:
        z (array_like): activation signal received by the layer.
    """
    def _sigmoid(self,z):
        # sigmoidal -> 1/(1 + e^-x)
        a = (1 + np.exp(-z))
        return 1/a

    """
    Computes de sigmoid derivation of de activation (private)

    Args:
        a (array_like): activation received by the layer.
    """   
    def _sigmoidPrime(self,a):
        return  a*(1 - a)

    """
    Run the feedwordwar neural network step

    Args:
        x (array_like): input of the neural network.

	Return 
	------
	a1,a2,a3 (array_like): activation functions of each layers
    z2,z3 (array_like): signal fuction of two last layers
    """
    def feedforward(self,x):
        # listas finales de as y zs
        a = []
        z = []

        # primera capa de neuronas (entrada)
        s = self._size(x)
        a1 = np.hstack([np.ones((s, 1)), x])
        a.append(a1)

        # el resto de thetas
        for theta in self.thetas:

            z_aux = a[-1] @ theta.T
            z.append(z_aux)
            a_aux = self._sigmoid(z_aux)
            s = self._size(a_aux)

            # hstack
            # si no es la ultima theta le mete el hstack
            if theta is not self.thetas[-1]:
                a_aux = np.hstack([np.ones((s, 1)), a_aux])
            a.append(a_aux)
            
        return a, z


    """
    Computes only the cost of a previously generated output (private)

    Args:
        yPrime (array_like): output generated by neural network.
        y (array_like): output from the dataset
        lambda_ (scalar): regularization parameter

	Return 
	------
	J (scalar): the cost.
    """
    def compute_cost(self, yPrime,y, lambda_): # es una función interna por eso empieza por _
        m = y.shape[0]
        J = (-1/m) * np.sum(y * np.log(yPrime) + (1 - y) * np.log(1 - yPrime))
        J += self._regularizationL2Cost(m, lambda_)
        return J
    

    """
    Get the class with highest activation value

    Args:
        a3 (array_like): output generated by neural network.

	Return 
	------
	p (scalar): the class index with the highest activation value.
    """
    def predict(self,a3):
        p = np.argmax(a3, axis=1)
        return p
    

    """
    Compute the gradients of both theta matrix parámeters and cost J

    Args:
        x (array_like): input of the neural network.
        y (array_like): output of the neural network.
        lambda_ (scalar): regularization.

	Return 
	------
	J: cost
    grad1, grad2: the gradient matrix (same shape than theta1 and theta2)
    """
    def compute_gradients(self, x, y, lambda_):

        #
        m = self._size(x)    
        a, z = self.feedforward(x)
        J = self.compute_cost(a[-1], y, lambda_)

        # lista de gradientes y de deltas
        gradientes = []
        deltas = []


        # error de la capa de salida
        past_delta = a[-1] - y
        deltas.append(past_delta)  

        # penultima capa
        past_delta = deltas[-1].T @ a[-2]  # Cogemos el penultimo elemento
        grad = past_delta / m
        reg = self._regularizationL2Gradient(self.thetas[-1], lambda_, m)
        grad += reg
        gradientes.append(grad)

        size = len(self.hiddenLayer)
        for i in range(size):
            j = len(self.hiddenLayer) - i

            # error delta
            delta_t = np.dot(past_delta, self.thetas[i+1].T)
            d = delta_t@self._sigmoidPrime(self._sigmoid(z[j])).T
            delta = d @ a[j]
            deltas.append(delta)

            #gradiente
            grad = deltas[-1]/m
            grad += self._regularizationL2Gradient(self.thetas[i], lambda_, m)
            gradientes += grad

            past_delta = d

        return J, gradientes
    
    """
    Compute L2 regularization gradient

    Args:
        theta (array_like): a theta matrix to calculate the regularization.
        lambda_ (scalar): regularization.
        m (scalar): the size of the X examples.

	Return 
	------
	L2 Gradient value
    """
    def _regularizationL2Gradient(self, theta, lambda_, m):
        ##TO-DO
        a = (lambda_/m) * np.copy(theta)
        return a[:, 1:]
    
    
    """
    Compute L2 regularization cost

    Args:
        lambda_ (scalar): regularization.
        m (scalar): the size of the X examples.

	Return 
	------
	L2 cost value
    """

    def _regularizationL2Cost(self, m, lambda_):
        #a = np.sum(self.theta1[:, 1:]**2)
        #b = np.sum(self.theta2[:, 1:]**2)
        # lo de arriba pero sum en un for okok
        cost = 0
        for theta in self.thetas:
            cost += np.sum(theta[:, 1:]**2)
        return (lambda_/(2*m)) * cost
    
    
    def backpropagation(self, x, y, alpha, lambda_, numIte, verbose=0):
        Jhistory = []
        for i in range(numIte):
            ##TO-DO: calculate gradients and update both theta matrix
            J, gradientes = self.compute_gradients(x, y, lambda_)
            #self.theta1 -= alpha * grad1
            #self.theta2 -= alpha * grad2
            for i in len(self.thetas):
                self.thetas[i] -= alpha * gradientes[i]
            Jhistory.append(J)
            if verbose > 0 :
                if i % verbose == 0 or i == (numIte-1):
                    print(f"Iteration {(i+1):6}: Cost {float(J):8.4f}   ")
        
        return Jhistory
    


def MLP_backprop_predict_complete(X_train,y_train, X_test, alpha, lambda_, num_ite, verbose):
     mlp = MLP_Complete(X_train.shape[1],[25],y_train.shape[1])
     Jhistory = mlp.backpropagation(X_train,y_train,alpha,lambda_,num_ite,verbose)
     a, z = mlp.feedforward(X_test)
     a3 = a[-1]             # coge la ultima
     y_pred=mlp.predict(a3)
     return y_pred